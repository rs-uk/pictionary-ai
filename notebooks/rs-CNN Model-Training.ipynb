{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f15db8-3ac3-4278-bf91-fb361f3ea408",
   "metadata": {},
   "source": [
    "# CNN Model for QuickDraw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d07152-4733-4ef2-be51-fc284e774287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import callbacks\n",
    "# from tensorflow.keras import utils\n",
    "# from keras import Model, Sequential, layers, regularizers, optimizers\n",
    "# from sklearn.preprocessing import TargetEncoder\n",
    "# from colorama import Fore, Style\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b08c93-5fe4-423a-8b01-9adfd588945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277b1cb-4242-4c1e-bdf6-3f46e48a47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "def initialize_model() -> Model:\n",
    "    '''\n",
    "    Initialise model with the same CNN structure we used for number recognition - accepting bmp files of dimension 28x28 bits\n",
    "    * `Conv2D` layer with 8 filters, each of size (4, 4), an input shape of (28x28), the `relu` activation function, and `padding='same'\n",
    "    * `MaxPool2D` layer with a `pool_size` equal to (2, 2)\n",
    "    * second `Conv2D` layer with 16 filters, each of size (3, 3), and the `relu` activation function\n",
    "    * second `MaxPool2D` layer with a `pool_size` equal to (2, 2)\n",
    "    \n",
    "    * `Flatten` layer\n",
    "    * first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "    * last softmax predictive layer of 50 classes - i.e. number of classes in data subset\n",
    "    '''\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(50, activation='softmax'))\n",
    "    \n",
    "    print(\"✅ Model initialized\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d359d55-e77a-41d4-9b74-d4f4dbca6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model: Model, learning_rate=0.0005) -> Model:\n",
    "    '''\n",
    "    Compile the model, which:\n",
    "    * optimizes the `categorical_crossentropy` loss function,\n",
    "    * with the `adam` optimizer with learning_rate=0.0005, \n",
    "    * and the `accuracy` as the metrics\n",
    "    '''\n",
    "\n",
    "    # Create optimizer with custom learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    print(\"✅ Model compiled\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8a1fa-0e4f-4f2c-a133-a095de3001c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary \n",
    "model = initialize_model()\n",
    "model = compile_model(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01664e34-7df6-47f7-85f0-7e10fe5824fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save json to local folder - to store training history\n",
    "def save_json_to_local (data: list, folder_path: str, file_name: str) -> None: \n",
    "    file_path = folder_path+file_name\n",
    "    with open(file_path, 'w') as file : \n",
    "        json.dump(data, file)\n",
    "    print(f'Saved data to {file_path}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4691d-ec64-426e-aa8c-67d6dabf8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "            model: Model,\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            batch_size=256,\n",
    "            patience=3,\n",
    "            validation_data=None, # overrides validation_split, if available\n",
    "            validation_split=0.2\n",
    "            ) -> Tuple[Model, dict]:\n",
    "    '''\n",
    "    Train on the model and return a tuple (fitted_model, history)\n",
    "    Checkpoints have also been included to store model weights after each epoch\n",
    "    ''' \n",
    "    \n",
    "    es = callbacks.EarlyStopping(\n",
    "            monitor=\"val_accuracy\",\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "            )\n",
    "    \n",
    "    # Create checkpoints\n",
    "    checkpoint_filepath = '/home/jupyter/lewagon_projects/pictionary-ai/raw_data/models_1003_50classes'\n",
    "    \n",
    "    #Save the checkpoints in the checkpoint_filepath\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True\n",
    "            )\n",
    "\n",
    "    history = model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            validation_data=validation_data,\n",
    "            validation_split=validation_split,\n",
    "            epochs=50,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[es, model_checkpoint_callback],\n",
    "            verbose=1\n",
    "            )\n",
    "    \n",
    "    print(f\"✅ Model trained on {len(X)} rows with maximum val accuracy: {round(np.min(history.history['accuracy']), 2)}\")\n",
    "\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    save_json_to_local (history, checkpoint_filepath, 'model_history_'+timestr)  \n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6bf88-cb7a-40b0-8d29-ea0409a06ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "            model: Model,\n",
    "            X: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            batch_size=64\n",
    "            ) -> Tuple[Model, dict]:\n",
    "    '''\n",
    "    Evaluate performance of the trained model on the test dataset\n",
    "    Returns evaluation metrics\n",
    "    '''\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"\\n❌ No model to evaluate\")\n",
    "        return None\n",
    "\n",
    "    metrics = model.evaluate(\n",
    "            x=X,\n",
    "            y=y,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            # callbacks=None,\n",
    "            return_dict=True\n",
    "            )\n",
    "\n",
    "    loss = metrics[\"loss\"]\n",
    "    accuracy = metrics[\"accuracy\"]\n",
    "\n",
    "    print(f\"✅ Model evaluated, accuracy: {round(accuracy, 2)}\")\n",
    "\n",
    "    return metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
